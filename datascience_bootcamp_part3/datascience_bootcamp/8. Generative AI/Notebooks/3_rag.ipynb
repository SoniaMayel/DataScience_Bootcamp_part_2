{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQEWtbLY3-xd"
      },
      "source": [
        "# Retrieval Augmented Generation (RAG)\n",
        "Retrieval Augmented Generation (RAG) combines information retrieval with language models. It first searches for relevant facts in external sources, then feeds those facts to the language model alongside the user's prompt. This helps the model generate more accurate and factual responses, even on topics beyond its initial training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO0LbZbd4HZ-"
      },
      "source": [
        "---\n",
        "## 1.&nbsp; Installations and Settings 🛠️\n",
        "Let's get started by setting up a GPU on your Colab notebook. Head over to `Edit > Notebook Settings` and select `GPU` from the runtime type dropdown. Once you've made that change, click `Save`.\n",
        "\n",
        "Now, we'll need to install two additional libraries to build our RAG model. These libraries will help us create and store numerical representations of our text, which are essential for this task.\n",
        "\n",
        "1. **sentence_transformers:** This library will generate embeddings, which are like numerical summaries of our text. These embeddings will allow us to compare different sentences and identify relationships between them.\n",
        "\n",
        "2. **faiss-gpu:** This library provides a fast and efficient database for storing and retrieving our numerical summaries.\n",
        "\n",
        "> If you're using a CPU instead of a GPU, install `faiss-cpu` instead. This version will work just fine, but it may be a bit slower."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ_oTkGwppsp",
        "outputId": "966b3e2f-2ace-42bd-a790-383fe97f2eff"
      },
      "outputs": [],
      "source": [
        "!pip3 install -qqq langchain --progress-bar off\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip3 install -qqq llama-cpp-python --progress-bar off\n",
        "\n",
        "!pip3 install -qqq sentence_transformers --progress-bar off\n",
        "!pip3 install -qqq faiss-gpu --progress-bar off\n",
        "\n",
        "!huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF mistral-7b-instruct-v0.1.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7d2-gAb5Cw7"
      },
      "source": [
        "---\n",
        "## 2.&nbsp; Setting up your LLM 🧠\n",
        "Here we'll add one extra parameter, `n_ctx`. This parameter controls the size of the input context. The larger the window, the more memory you're likely to use. The default of 512 is fine for most basic things, but as we are starting to retrieve articles and add them to the context window, we'll double the size to 1024. Feel free to play around with this number as your project needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-F76VzycqhLv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/Chapter_8_generative_Ai/Model/faiss_index/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
            "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  4095.08 MiB, ( 4095.14 / 10922.67)\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
            "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 1024\n",
            "llama_new_context_with_model: n_batch    = 8\n",
            "llama_new_context_with_model: n_ubatch   = 8\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "ggml_metal_init: allocating\n",
            "ggml_metal_init: found device: Apple M2\n",
            "ggml_metal_init: picking default device: Apple M2\n",
            "ggml_metal_init: using embedded metal library\n",
            "ggml_metal_init: GPU name:   Apple M2\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
            "ggml_metal_init: simdgroup reduction support   = true\n",
            "ggml_metal_init: simdgroup matrix mul. support = true\n",
            "ggml_metal_init: hasUnifiedMemory              = true\n",
            "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
            "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   128.00 MiB, ( 4224.95 / 10922.67)\n",
            "llama_kv_cache_init:      Metal KV buffer size =   128.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     1.55 MiB, ( 4226.50 / 10922.67)\n",
            "llama_new_context_with_model:      Metal compute buffer size =     1.53 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =     0.16 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.1'}\n",
            "Using fallback chat format: None\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "\n",
        "llm = LlamaCpp(model_path = \"/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/Chapter_8_generative_Ai/Model/faiss_index/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
        "               max_tokens = 2000,\n",
        "               temperature = 0.1,\n",
        "               top_p = 1,\n",
        "               n_gpu_layers = -1,\n",
        "               n_ctx = 1024)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWG2XPn15NgV"
      },
      "source": [
        "\n",
        "### 2.1.&nbsp;  Test your LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fc8mzzu51d5b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =    6789.94 ms\n",
            "llama_print_timings:      sample time =      17.75 ms /   165 runs   (    0.11 ms per token,  9294.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6789.65 ms /     8 tokens (  848.71 ms per token,     1.18 tokens per second)\n",
            "llama_print_timings:        eval time =   10219.85 ms /   164 runs   (   62.32 ms per token,    16.05 tokens per second)\n",
            "llama_print_timings:       total time =   17278.71 ms /   172 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Data Science is the future, it's where we're all headed,\n",
            "With algorithms and models, predictions are guaranteed.\n",
            "We take in information from all around,\n",
            "And use it to create insights that astound.\n",
            "\n",
            "From machine learning to deep learning too,\n",
            "There's nothing that data science can't do.\n",
            "It helps us make decisions with ease,\n",
            "And find patterns that were once hard to see.\n",
            "\n",
            "With data science, we can predict the past,\n",
            "And even shape the future that's yet to come.\n",
            "It's a field that's constantly evolving,\n",
            "And with every new discovery, our world is improving.\n",
            "\n",
            "So let's embrace this future that's bright,\n",
            "And use data science to make everything right.\n"
          ]
        }
      ],
      "source": [
        "answer_1 = llm.invoke(\"Write a poem about data science.\")\n",
        "print(answer_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCif7c-y9hdw"
      },
      "source": [
        "---\n",
        "## 3.&nbsp; Retrieval Augmented Generation 🔃"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSdu2iln-Gi8"
      },
      "source": [
        "### 3.1.&nbsp; Find your data\n",
        "Our model needs some information to work its magic! In this case, we'll be using a copy of Alice's Adventures in Wonderland, but feel free to swap it out for anything you like: legal documents, school textbooks, websites – the possibilities are endless!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#run in the terminal\n",
        "pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ghvsh8ag8-Ts"
      },
      "outputs": [],
      "source": [
        "!wget -O alice_in_wonderland.txt https://www.gutenberg.org/cache/epub/11/pg11.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwoxAMLS9p_Z"
      },
      "source": [
        "> If your working locally, just download a txt book from Project Gutenburg. Here's the link to [Alice's Adventures in Wonderland](https://www.gutenberg.org/cache/epub/11/pg11.txt). Feel free to use any other book though."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MKSMLO5-KN-"
      },
      "source": [
        "### 3.2.&nbsp; Load the data\n",
        "Now that we have the data, we have to load it in a format LangChain can understand. For this, Langchain has [loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/). There's loaders for CSV, text, PDF, and a host of other formats. You're not restricted to just text here.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gYIfLLbIbkly"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\"/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/Chapter_8_generative_Ai/Model/faiss_index/alice_in_wonderland.txt\")\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3PsFsW6-L-a"
      },
      "source": [
        "### 3.3.&nbsp; Splitting the document\n",
        "Obviously, a whole book is a lot to digest. This is made easier by [splitting](https://python.langchain.com/docs/modules/data_connection/document_transformers/) the document into chunks. You can split it by paragraphs, sentences, or even individual words, depending on what you want to analyse. In Langchain, we have different tools like the RecursiveCharacterTextSplitter (say that five times fast!) that understand the structure of text and help you break it down into manageable chunks.\n",
        "\n",
        "Check out [this website](https://chunkviz.up.railway.app/) to help visualise the splitting process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FdwvsTy0bkjH"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=800,\n",
        "                                               chunk_overlap=150)\n",
        "\n",
        "docs = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbNC8zhE-PcN"
      },
      "source": [
        "### 3.4.&nbsp; Creating vectors with embeddings\n",
        "\n",
        "[Embeddings](https://python.langchain.com/docs/integrations/text_embedding) are a fancy way of saying we turn words into numbers that computers can understand. Each word gets its own unique code, based on its meaning and relationship to other words. The list of numbers produced is known as a vector. Vectors allow us to compare text and find chunks that contain similar information.\n",
        "\n",
        "Different embedding models encode words and meanings in different ways, and finding the right one can be tricky. We're using open-source models from HuggingFace, who even have a handy [leaderboard of embeddings](https://huggingface.co/spaces/mteb/leaderboard) on their website. Just browse the options and see which one speaks your language (literally!).\n",
        "> As we are doing a retrieval project, click on the `Retrieval` tab of the leaderboard to see the best embeddings for retrieval tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7938cf7dd8da4dfe829f58de3053d77d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b62912c713b4c1da6f77eb9e5776eb2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e7c2432da46496193d96592c1bb1d40",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8c04d44cb55488082417891edfff42d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "38fdc5284a4e40dbba3ebf4ca3ba6787",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59daa9171665456eb33c8569fb48b578",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eaf4817eca3642eab048456bad9a7d2a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4c8cfc1aed746859396b2bf0f4f58c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9dbe5021d8c7467abdb1d9b6420307ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "334d164477be424bb5627d68294ad72b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2608f18421444f1d83308dd0a9b24cec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import os\n",
        "\n",
        "embedding_model = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "current_folder = os.getcwd()\n",
        "embeddings_folder = current_folder  # Using the current directory\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model, cache_folder=embeddings_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/Chapter_8_generative_Ai/class_notebooks\n"
          ]
        }
      ],
      "source": [
        "current_folder = os.getcwd()\n",
        "print(current_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBhmJNSMNPt7"
      },
      "source": [
        "> The first time you use the embeddings model, it'll download all the necessary data. After that it runs locally on your machine. Unfortunately, Colab is a different story – its sessions end, so the model needs to download again each time.\n",
        "\n",
        "To exemplify using embeddings to transform a sentence into a vector, let's look at an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2SzwAVG30JfV"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.009409726597368717,\n",
              " -0.023806331679224968,\n",
              " -0.012127568945288658,\n",
              " 0.03612378612160683,\n",
              " -0.03382447361946106,\n",
              " -0.07974186539649963,\n",
              " 0.07004600018262863,\n",
              " 0.0746554359793663,\n",
              " 0.04014184698462486,\n",
              " 0.04419058933854103,\n",
              " -0.007505984045565128,\n",
              " -0.060012150555849075,\n",
              " -0.10028237104415894,\n",
              " 0.032309673726558685,\n",
              " -0.039546284824609756,\n",
              " 0.016906695440411568,\n",
              " -0.0303138867020607,\n",
              " -0.12780103087425232,\n",
              " -0.03218212351202965,\n",
              " -0.07546596229076385,\n",
              " 7.660531264264137e-05,\n",
              " 0.05085941031575203,\n",
              " 0.12591633200645447,\n",
              " -0.04004546254873276,\n",
              " 0.040401391685009,\n",
              " -0.022957686334848404,\n",
              " -0.07265669852495193,\n",
              " -0.02543501742184162,\n",
              " -0.01982485130429268,\n",
              " 0.011819676496088505,\n",
              " -0.035723403096199036,\n",
              " 0.036577362567186356,\n",
              " 0.07559998333454132,\n",
              " 0.03425060212612152,\n",
              " -0.05330543592572212,\n",
              " -0.030826333910226822,\n",
              " 0.02147846296429634,\n",
              " 0.12243172526359558,\n",
              " -0.0054456680081784725,\n",
              " 0.04834012687206268,\n",
              " -0.004316539969295263,\n",
              " -0.043691281229257584,\n",
              " 0.009050648659467697,\n",
              " 0.027110163122415543,\n",
              " 0.012248445302248001,\n",
              " -0.10022237151861191,\n",
              " 0.016736773774027824,\n",
              " 0.02665060944855213,\n",
              " 0.008149255067110062,\n",
              " 0.064472496509552,\n",
              " -0.1361851692199707,\n",
              " -0.014375676400959492,\n",
              " 0.009367618709802628,\n",
              " -0.054481372237205505,\n",
              " -0.06971157342195511,\n",
              " 0.045573510229587555,\n",
              " 0.03519922122359276,\n",
              " 0.07449532300233841,\n",
              " 0.0318605974316597,\n",
              " 0.053021010011434555,\n",
              " 0.010595540516078472,\n",
              " -0.058914776891469955,\n",
              " -0.04218410700559616,\n",
              " 0.02002161554992199,\n",
              " 0.06526479870080948,\n",
              " -0.0019041477935388684,\n",
              " 0.032197222113609314,\n",
              " 0.028809277340769768,\n",
              " -0.055399708449840546,\n",
              " 0.04273242503404617,\n",
              " -0.002228609984740615,\n",
              " -0.012309771962463856,\n",
              " -0.011138838715851307,\n",
              " 0.1012556403875351,\n",
              " 0.04594092071056366,\n",
              " 0.04096715897321701,\n",
              " -0.006781568750739098,\n",
              " 0.02759375236928463,\n",
              " 0.0847785547375679,\n",
              " -0.01409639697521925,\n",
              " 0.07889802753925323,\n",
              " -0.12069518119096756,\n",
              " -0.02613339014351368,\n",
              " 0.05883386358618736,\n",
              " 0.03125665709376335,\n",
              " -0.020522018894553185,\n",
              " -0.022634027525782585,\n",
              " 0.00030224802321754396,\n",
              " -0.049353018403053284,\n",
              " -0.07062903046607971,\n",
              " -0.02058926410973072,\n",
              " -0.019274478778243065,\n",
              " 0.05455070734024048,\n",
              " 0.012043523602187634,\n",
              " -0.03527412191033363,\n",
              " 0.02070103958249092,\n",
              " -0.031046856194734573,\n",
              " -0.01984262280166149,\n",
              " 0.045814137905836105,\n",
              " 0.06683222204446793,\n",
              " 0.017091654241085052,\n",
              " -0.006984881591051817,\n",
              " 0.04995705559849739,\n",
              " -0.020615749061107635,\n",
              " -0.011824299581348896,\n",
              " -0.025338957086205482,\n",
              " -0.0008886861614882946,\n",
              " -0.03393888100981712,\n",
              " 0.04215676337480545,\n",
              " -0.03811667114496231,\n",
              " -0.0029110030736774206,\n",
              " 0.08240555971860886,\n",
              " 0.0723038986325264,\n",
              " -0.008444061502814293,\n",
              " 0.08958085626363754,\n",
              " -0.03419233113527298,\n",
              " -0.10964814573526382,\n",
              " -0.003884939942508936,\n",
              " 0.03722620755434036,\n",
              " -0.012584911659359932,\n",
              " 0.05145321041345596,\n",
              " 0.032286230474710464,\n",
              " -0.06588147580623627,\n",
              " 0.020802950486540794,\n",
              " 0.10875219106674194,\n",
              " 0.007592369336634874,\n",
              " -0.09537405520677567,\n",
              " -2.3699097525889983e-33,\n",
              " 0.024889299646019936,\n",
              " 0.060195982456207275,\n",
              " 0.04316581413149834,\n",
              " 0.003689653007313609,\n",
              " -0.03218454867601395,\n",
              " 0.012413411401212215,\n",
              " -0.09677629917860031,\n",
              " -0.03213798999786377,\n",
              " -0.05191207304596901,\n",
              " -0.036298975348472595,\n",
              " -0.017077166587114334,\n",
              " 0.030495742335915565,\n",
              " -0.03306036442518234,\n",
              " 0.018733156844973564,\n",
              " -0.029483361169695854,\n",
              " 0.12275638431310654,\n",
              " -0.03546414524316788,\n",
              " -0.03419365733861923,\n",
              " -0.04230489581823349,\n",
              " 0.02047664299607277,\n",
              " 0.023305954411625862,\n",
              " 0.000789591867942363,\n",
              " 0.07537371665239334,\n",
              " 0.011412079446017742,\n",
              " -0.026504382491111755,\n",
              " 0.028960853815078735,\n",
              " 0.03420186787843704,\n",
              " -0.010710667818784714,\n",
              " 0.04709921404719353,\n",
              " 0.020683985203504562,\n",
              " -0.10949775576591492,\n",
              " 0.012484787032008171,\n",
              " -0.03326398879289627,\n",
              " -0.04095077142119408,\n",
              " 0.08581794053316116,\n",
              " -0.028387172147631645,\n",
              " 0.008844674564898014,\n",
              " -0.024843210354447365,\n",
              " 0.0557437390089035,\n",
              " 0.07883001118898392,\n",
              " -0.04116813465952873,\n",
              " 0.02431868202984333,\n",
              " -0.01578664779663086,\n",
              " 0.07553298026323318,\n",
              " 0.01532020978629589,\n",
              " 0.05019087716937065,\n",
              " 0.013212988153100014,\n",
              " 0.005743629764765501,\n",
              " 0.04997985437512398,\n",
              " 0.07013566046953201,\n",
              " -0.022555582225322723,\n",
              " 0.0016085782554000616,\n",
              " 0.09479131549596786,\n",
              " -0.019930947571992874,\n",
              " 0.06767921894788742,\n",
              " 0.030198711901903152,\n",
              " 0.04465653374791145,\n",
              " -0.05308077484369278,\n",
              " 0.04896782711148262,\n",
              " -0.058824822306632996,\n",
              " -0.008684682659804821,\n",
              " 0.07645931094884872,\n",
              " -0.04175131022930145,\n",
              " 0.041629038751125336,\n",
              " -0.06086704879999161,\n",
              " 0.05792565643787384,\n",
              " -0.02377755381166935,\n",
              " 0.017937293276190758,\n",
              " 0.026417041197419167,\n",
              " 0.04252827912569046,\n",
              " 0.04337466135621071,\n",
              " 0.02276713028550148,\n",
              " 0.006968535482883453,\n",
              " -0.07505753636360168,\n",
              " -0.059790946543216705,\n",
              " -0.03241990506649017,\n",
              " -0.05139961838722229,\n",
              " -0.0020425450056791306,\n",
              " -0.008911733515560627,\n",
              " -0.005509675946086645,\n",
              " 0.07394663244485855,\n",
              " -0.11442052572965622,\n",
              " -0.04445242881774902,\n",
              " -0.10458815097808838,\n",
              " -0.03749217838048935,\n",
              " -0.07235665619373322,\n",
              " -0.006939969025552273,\n",
              " -0.028158001601696014,\n",
              " 0.009442434646189213,\n",
              " 0.002752578817307949,\n",
              " -0.02685287967324257,\n",
              " -0.009806456975638866,\n",
              " 0.013658314943313599,\n",
              " -0.04508911818265915,\n",
              " -0.003375752130523324,\n",
              " 1.5342759815153814e-33,\n",
              " -0.11070341616868973,\n",
              " 0.06655651330947876,\n",
              " -0.1019810363650322,\n",
              " 0.12452490627765656,\n",
              " 0.019078826531767845,\n",
              " -0.004909857641905546,\n",
              " 0.043206837028265,\n",
              " -0.08586539328098297,\n",
              " 0.09091564267873764,\n",
              " -0.006308809854090214,\n",
              " -0.007707057520747185,\n",
              " -0.034286245703697205,\n",
              " -0.027026668190956116,\n",
              " -0.12129206210374832,\n",
              " 0.053304046392440796,\n",
              " -0.03094984032213688,\n",
              " 0.054708994925022125,\n",
              " -0.014792882837355137,\n",
              " -0.10676510632038116,\n",
              " -0.06137333810329437,\n",
              " 0.039644695818424225,\n",
              " 0.09239403158426285,\n",
              " -0.06888388097286224,\n",
              " 0.047746460884809494,\n",
              " 0.0012110413517802954,\n",
              " 0.011389261111617088,\n",
              " 0.005016987212002277,\n",
              " -0.08627907186746597,\n",
              " -0.0782250165939331,\n",
              " 0.014460660517215729,\n",
              " -0.08576064556837082,\n",
              " 0.006980275269597769,\n",
              " -0.0553283765912056,\n",
              " -0.039880573749542236,\n",
              " -0.019436240196228027,\n",
              " 0.0011094810906797647,\n",
              " 0.024691497907042503,\n",
              " -0.04105382412672043,\n",
              " -0.04461283981800079,\n",
              " -0.026788998395204544,\n",
              " -0.010827533900737762,\n",
              " 0.01322552002966404,\n",
              " -0.02666599676012993,\n",
              " -0.00804550014436245,\n",
              " 0.06636935472488403,\n",
              " 0.009695558808743954,\n",
              " -0.01857641525566578,\n",
              " 0.007657946553081274,\n",
              " -0.023577040061354637,\n",
              " 0.018438495695590973,\n",
              " 0.0616970956325531,\n",
              " -0.0007458834443241358,\n",
              " -0.00447434326633811,\n",
              " -0.05938759446144104,\n",
              " 0.009270974434912205,\n",
              " -0.04664243012666702,\n",
              " 0.055166132748126984,\n",
              " -0.006301292683929205,\n",
              " -0.09123237431049347,\n",
              " 0.00692865950986743,\n",
              " -0.047249507158994675,\n",
              " -0.08848708122968674,\n",
              " 0.06729693710803986,\n",
              " -0.005495188757777214,\n",
              " -0.052237726747989655,\n",
              " -0.08965378999710083,\n",
              " 0.020215097814798355,\n",
              " 0.03786221891641617,\n",
              " 0.02067239210009575,\n",
              " -0.04660971835255623,\n",
              " 0.0676095113158226,\n",
              " -0.028521334752440453,\n",
              " -0.012203921563923359,\n",
              " -0.0007568887667730451,\n",
              " -0.06788415461778641,\n",
              " 0.08346229791641235,\n",
              " -0.05922466143965721,\n",
              " -0.006041997112333775,\n",
              " -0.05536435917019844,\n",
              " 0.005111271981149912,\n",
              " -0.01347661018371582,\n",
              " -0.11282900720834732,\n",
              " 0.12649555504322052,\n",
              " 0.06240753084421158,\n",
              " 0.03518889844417572,\n",
              " 0.06333795189857483,\n",
              " -0.0628681480884552,\n",
              " 0.02542411908507347,\n",
              " -0.07000292092561722,\n",
              " 0.09257040917873383,\n",
              " -0.005658241920173168,\n",
              " -0.04041097313165665,\n",
              " -0.06954431533813477,\n",
              " 0.0321936160326004,\n",
              " 0.03288594260811806,\n",
              " -1.9684780028228488e-08,\n",
              " 0.04113159701228142,\n",
              " -0.02891526371240616,\n",
              " -0.02842523530125618,\n",
              " -0.03152799233794212,\n",
              " 0.046172529458999634,\n",
              " -0.05969003960490227,\n",
              " -0.024962177500128746,\n",
              " 0.07096164673566818,\n",
              " 0.04897993430495262,\n",
              " 0.007451685611158609,\n",
              " 0.08206308633089066,\n",
              " 0.055244702845811844,\n",
              " 0.0348394438624382,\n",
              " 0.07213205844163895,\n",
              " 0.05623047426342964,\n",
              " 0.06600040942430496,\n",
              " 0.1147649884223938,\n",
              " -0.0052842204459011555,\n",
              " -0.06813012063503265,\n",
              " 0.027961453422904015,\n",
              " -0.01753399521112442,\n",
              " 0.05422030761837959,\n",
              " -0.02828587032854557,\n",
              " 0.01815473474562168,\n",
              " -0.03028804436326027,\n",
              " 0.001460048952139914,\n",
              " -0.05541820824146271,\n",
              " 0.024853676557540894,\n",
              " -0.021405035629868507,\n",
              " 0.07860042154788971,\n",
              " -0.029634177684783936,\n",
              " 0.012791875749826431,\n",
              " -0.037763938307762146,\n",
              " 0.018781445920467377,\n",
              " 0.05013345181941986,\n",
              " -0.008437609300017357,\n",
              " 0.06195094808936119,\n",
              " 0.033109985291957855,\n",
              " 0.05188595503568649,\n",
              " -0.012250479310750961,\n",
              " -0.10175289958715439,\n",
              " 0.007577894255518913,\n",
              " 0.019991181790828705,\n",
              " 0.07500091940164566,\n",
              " -0.014996754005551338,\n",
              " -0.006573692429810762,\n",
              " -0.027998685836791992,\n",
              " 0.020182879641652107,\n",
              " 0.045054059475660324,\n",
              " 0.030877534300088882,\n",
              " 0.004332521464675665,\n",
              " -0.023936470970511436,\n",
              " -0.06647086888551712,\n",
              " -0.015604170970618725,\n",
              " 0.029573559761047363,\n",
              " -0.03752639889717102,\n",
              " 0.006519836373627186,\n",
              " 0.012791255488991737,\n",
              " -0.1377393901348114,\n",
              " -0.028690818697214127,\n",
              " 0.07609738409519196,\n",
              " -0.011266806162893772,\n",
              " 0.052615828812122345,\n",
              " -0.07821525633335114]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_text = \"Why do data scientists make great comedians? They're always trying to make ANOVA pun\"\n",
        "query_result = embeddings.embed_query(test_text)\n",
        "query_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nB9NFaPg2fYB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The 84 character sentence was transformed into a 384 dimension vector\n"
          ]
        }
      ],
      "source": [
        "characters = len(test_text)\n",
        "dimensions = len(query_result)\n",
        "print(f\"The {characters} character sentence was transformed into a {dimensions} dimension vector\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12HrYsvv8-ae"
      },
      "source": [
        "Embedding vectors have a fixed length, meaning each vector produced by this specific embedding will always have 384 dimensions. Choosing the appropriate embedding size involves a trade-off between accuracy and computational efficiency. Larger embeddings capture more semantic information but require more memory and processing power. Start with the provided MiniLM embedding as a baseline and experiment with different sizes to find the optimal balance for your needs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99m06CuX0CBt"
      },
      "source": [
        "### 3.5.&nbsp; Creating a vector database\n",
        "Imagine a library where books aren't just filed alphabetically, but also by their themes, characters, and emotions. That's the magic of vector databases: they unlock information beyond keywords, connecting ideas in unexpected ways."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XC74KL_Mbkga"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "vector_db = FAISS.from_documents(docs, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i95GEsABGfag"
      },
      "source": [
        "Once the database is made, you can save it to use over and over again in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SSdCx7nCEZ_1"
      },
      "outputs": [],
      "source": [
        "vector_db.save_local(\"/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/Chapter_8_generative_Ai/Model/vector\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GL9iRgt_GlQw"
      },
      "source": [
        "Here's the code to load it again.\n",
        "\n",
        "We'll leave it commented out here as we don't need it right now - it's already stored above in the variable `vector_db`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2HCPo-WEiiE"
      },
      "outputs": [],
      "source": [
        "# new_db = FAISS.load_local(\"/content/faiss_index\", embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9C1-lCpGq4T"
      },
      "source": [
        "You can also search your database to see which vectors are close to your input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WzQuiYGQEzja"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='CHAPTER VII.\\nA Mad Tea-Party\\n\\n\\nThere was a table set out under a tree in front of the house, and the\\nMarch Hare and the Hatter were having tea at it: a Dormouse was sitting\\nbetween them, fast asleep, and the other two were using it as a\\ncushion, resting their elbows on it, and talking over its head. “Very\\nuncomfortable for the Dormouse,” thought Alice; “only, as it’s asleep,\\nI suppose it doesn’t mind.”\\n\\nThe table was a large one, but the three were all crowded together at\\none corner of it: “No room! No room!” they cried out when they saw\\nAlice coming. “There’s _plenty_ of room!” said Alice indignantly, and\\nshe sat down in a large arm-chair at one end of the table.\\n\\n“Have some wine,” the March Hare said in an encouraging tone.', metadata={'source': '/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/Chapter_8_generative_Ai/Model/faiss_index/alice_in_wonderland.txt'}),\n",
              " Document(page_content='“I’d rather finish my tea,” said the Hatter, with an anxious look at\\nthe Queen, who was reading the list of singers.\\n\\n“You may go,” said the King, and the Hatter hurriedly left the court,\\nwithout even waiting to put his shoes on.\\n\\n“—and just take his head off outside,” the Queen added to one of the\\nofficers: but the Hatter was out of sight before the officer could get\\nto the door.\\n\\n“Call the next witness!” said the King.\\n\\nThe next witness was the Duchess’s cook. She carried the pepper-box in\\nher hand, and Alice guessed who it was, even before she got into the\\ncourt, by the way the people near the door began sneezing all at once.\\n\\n“Give your evidence,” said the King.\\n\\n“Shan’t,” said the cook.', metadata={'source': '/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/Chapter_8_generative_Ai/Model/faiss_index/alice_in_wonderland.txt'}),\n",
              " Document(page_content='All this time the Queen had never left off staring at the Hatter, and,\\njust as the Dormouse crossed the court, she said to one of the officers\\nof the court, “Bring me the list of the singers in the last concert!”\\non which the wretched Hatter trembled so, that he shook both his shoes\\noff.\\n\\n“Give your evidence,” the King repeated angrily, “or I’ll have you\\nexecuted, whether you’re nervous or not.”\\n\\n“I’m a poor man, your Majesty,” the Hatter began, in a trembling voice,\\n“—and I hadn’t begun my tea—not above a week or so—and what with the\\nbread-and-butter getting so thin—and the twinkling of the tea—”\\n\\n“The twinkling of the _what?_” said the King.\\n\\n“It _began_ with the tea,” the Hatter replied.', metadata={'source': '/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/Chapter_8_generative_Ai/Model/faiss_index/alice_in_wonderland.txt'}),\n",
              " Document(page_content='“The twinkling of the _what?_” said the King.\\n\\n“It _began_ with the tea,” the Hatter replied.\\n\\n“Of course twinkling begins with a T!” said the King sharply. “Do you\\ntake me for a dunce? Go on!”\\n\\n“I’m a poor man,” the Hatter went on, “and most things twinkled after\\nthat—only the March Hare said—”\\n\\n“I didn’t!” the March Hare interrupted in a great hurry.\\n\\n“You did!” said the Hatter.\\n\\n“I deny it!” said the March Hare.\\n\\n“He denies it,” said the King: “leave out that part.”\\n\\n“Well, at any rate, the Dormouse said—” the Hatter went on, looking\\nanxiously round to see if he would deny it too: but the Dormouse denied\\nnothing, being fast asleep.\\n\\n“After that,” continued the Hatter, “I cut some more bread-and-butter—”\\n\\n“But what did the Dormouse say?” one of the jury asked.', metadata={'source': '/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/Chapter_8_generative_Ai/Model/faiss_index/alice_in_wonderland.txt'})]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_db.similarity_search(\"What does the Mad Hatter drink?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve31AV92-T_C"
      },
      "source": [
        "### 3.6.&nbsp; Adding a prompt\n",
        "We can guide our model's behavior with a prompt, similar to how we gave instructions to the chatbot. We'll use specific tags in the prompt to tell the model what to do. These tags, `<s> </s>` and `[INST] [/INST]`, come straight from the [model's documentation on Hugging Face](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF). They can also be found in [Mistral's docs](https://docs.mistral.ai/models/). Different models have different expectations for prompts, so always check the documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "J1LsKEpwbkXI"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "input_template = \"\"\"\n",
        "<s>\n",
        "[INST] Answer the question based only on the following context: [/INST]\n",
        "{context}\n",
        "</s>\n",
        "[INST] Question: {question} [/INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=input_template,\n",
        "                        input_variables=[\"context\", \"question\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e86UMPFwE_rB"
      },
      "source": [
        "### 3.7.&nbsp; RAG - chaining it all together\n",
        "This is the final piece of the puzzle, we now bring everything together in a chain. Our vector database, our prompt, and our LLM join to give us retrieval augmented generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9JhOZwcl8-NX"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=vector_db.as_retriever(search_kwargs={\"k\": 2}), # top 2 results only, speed things up\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "cwiG3ZKkL1TA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    6789.94 ms\n",
            "llama_print_timings:      sample time =      15.42 ms /    50 runs   (    0.31 ms per token,  3242.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =   16358.67 ms /   472 tokens (   34.66 ms per token,    28.85 tokens per second)\n",
            "llama_print_timings:        eval time =    3230.55 ms /    49 runs   (   65.93 ms per token,    15.17 tokens per second)\n",
            "llama_print_timings:       total time =   19998.08 ms /   521 tokens\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': 'Who likes to chop off heads?',\n",
              " 'result': 'The Queen in the story \"Alice\\'s Adventures in Wonderland\" likes to chop off heads. She orders her guards to execute two gardeners who were tending to a rose-tree by saying, \"Off with their heads!\"',\n",
              " 'source_documents': [Document(page_content='“Leave off that!” screamed the Queen. “You make me giddy.” And then,\\nturning to the rose-tree, she went on, “What _have_ you been doing\\nhere?”\\n\\n“May it please your Majesty,” said Two, in a very humble tone, going\\ndown on one knee as he spoke, “we were trying—”\\n\\n“_I_ see!” said the Queen, who had meanwhile been examining the roses.\\n“Off with their heads!” and the procession moved on, three of the\\nsoldiers remaining behind to execute the unfortunate gardeners, who ran\\nto Alice for protection.\\n\\n“You shan’t be beheaded!” said Alice, and she put them into a large\\nflower-pot that stood near. The three soldiers wandered about for a\\nminute or two, looking for them, and then quietly marched off after the\\nothers.\\n\\n“Are their heads off?” shouted the Queen.', metadata={'source': '/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/Chapter_8_generative_Ai/Model/faiss_index/alice_in_wonderland.txt'}),\n",
              "  Document(page_content='“In my youth,” said the sage, as he shook his grey locks,\\n    “I kept all my limbs very supple\\nBy the use of this ointment—one shilling the box—\\n    Allow me to sell you a couple?”\\n\\n“You are old,” said the youth, “and your jaws are too weak\\n    For anything tougher than suet;\\nYet you finished the goose, with the bones and the beak—\\n    Pray, how did you manage to do it?”\\n\\n“In my youth,” said his father, “I took to the law,\\n    And argued each case with my wife;\\nAnd the muscular strength, which it gave to my jaw,\\n    Has lasted the rest of my life.”\\n\\n“You are old,” said the youth, “one would hardly suppose\\n    That your eye was as steady as ever;\\nYet you balanced an eel on the end of your nose—\\n    What made you so awfully clever?”', metadata={'source': '/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/Chapter_8_generative_Ai/Model/faiss_index/alice_in_wonderland.txt'})]}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answer = qa_chain.invoke(\"Who likes to chop off heads?\")\n",
        "\n",
        "answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    6789.94 ms\n",
            "llama_print_timings:      sample time =       2.49 ms /    26 runs   (    0.10 ms per token, 10441.77 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_print_timings:        eval time =    1949.50 ms /    26 runs   (   74.98 ms per token,    13.34 tokens per second)\n",
            "llama_print_timings:       total time =    1990.39 ms /    27 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Queen orders the executioner to chop off the heads of the two gardeners who were tending to the roses.\n"
          ]
        }
      ],
      "source": [
        "print(qa_chain.invoke(\"Whose head does she chop off?\")['result'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzbVPImoQKuM"
      },
      "source": [
        "#### 3.7.1.&nbsp; Exploring the returned dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "j5qPkIFpQS5T"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['query', 'result', 'source_documents'])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answer.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDHJtk3SQEtx"
      },
      "source": [
        "##### `query`\n",
        "\n",
        "The question that we asked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "sYG-m40m8-Kp"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Who likes to chop off heads?'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answer['query']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0--crcoQQG4e"
      },
      "source": [
        "##### `result`\n",
        "\n",
        "The response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "eQQXlEfS89xC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The Queen in \"Alice\\'s Adventures in Wonderland\" likes to chop off heads. She orders her guards to behead two gardeners who were tending to a rose tree.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answer['result']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kEcB-6nk9Y47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Queen in \"Alice's Adventures in Wonderland\" likes to chop off heads. She orders her guards to behead two gardeners who were tending to a rose tree.\n"
          ]
        }
      ],
      "source": [
        "print(answer['result'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhxTs6IKQJGN"
      },
      "source": [
        "##### `source_documents`\n",
        "\n",
        "What information was used to form the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "pmYjIGvc8_XC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='“Leave off that!” screamed the Queen. “You make me giddy.” And then,\\nturning to the rose-tree, she went on, “What _have_ you been doing\\nhere?”\\n\\n“May it please your Majesty,” said Two, in a very humble tone, going\\ndown on one knee as he spoke, “we were trying—”\\n\\n“_I_ see!” said the Queen, who had meanwhile been examining the roses.\\n“Off with their heads!” and the procession moved on, three of the\\nsoldiers remaining behind to execute the unfortunate gardeners, who ran\\nto Alice for protection.\\n\\n“You shan’t be beheaded!” said Alice, and she put them into a large\\nflower-pot that stood near. The three soldiers wandered about for a\\nminute or two, looking for them, and then quietly marched off after the\\nothers.\\n\\n“Are their heads off?” shouted the Queen.', metadata={'source': '/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/alice_in_wonderland.txt'}),\n",
              " Document(page_content='“In my youth,” said the sage, as he shook his grey locks,\\n    “I kept all my limbs very supple\\nBy the use of this ointment—one shilling the box—\\n    Allow me to sell you a couple?”\\n\\n“You are old,” said the youth, “and your jaws are too weak\\n    For anything tougher than suet;\\nYet you finished the goose, with the bones and the beak—\\n    Pray, how did you manage to do it?”\\n\\n“In my youth,” said his father, “I took to the law,\\n    And argued each case with my wife;\\nAnd the muscular strength, which it gave to my jaw,\\n    Has lasted the rest of my life.”\\n\\n“You are old,” said the youth, “one would hardly suppose\\n    That your eye was as steady as ever;\\nYet you balanced an eel on the end of your nose—\\n    What made you so awfully clever?”', metadata={'source': '/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/alice_in_wonderland.txt'})]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answer['source_documents']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0R2VVZAE9B6g"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='“Leave off that!” screamed the Queen. “You make me giddy.” And then,\\nturning to the rose-tree, she went on, “What _have_ you been doing\\nhere?”\\n\\n“May it please your Majesty,” said Two, in a very humble tone, going\\ndown on one knee as he spoke, “we were trying—”\\n\\n“_I_ see!” said the Queen, who had meanwhile been examining the roses.\\n“Off with their heads!” and the procession moved on, three of the\\nsoldiers remaining behind to execute the unfortunate gardeners, who ran\\nto Alice for protection.\\n\\n“You shan’t be beheaded!” said Alice, and she put them into a large\\nflower-pot that stood near. The three soldiers wandered about for a\\nminute or two, looking for them, and then quietly marched off after the\\nothers.\\n\\n“Are their heads off?” shouted the Queen.', metadata={'source': '/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/alice_in_wonderland.txt'})"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answer['source_documents'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xa8eSQAz9DrM"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'“Leave off that!” screamed the Queen. “You make me giddy.” And then,\\nturning to the rose-tree, she went on, “What _have_ you been doing\\nhere?”\\n\\n“May it please your Majesty,” said Two, in a very humble tone, going\\ndown on one knee as he spoke, “we were trying—”\\n\\n“_I_ see!” said the Queen, who had meanwhile been examining the roses.\\n“Off with their heads!” and the procession moved on, three of the\\nsoldiers remaining behind to execute the unfortunate gardeners, who ran\\nto Alice for protection.\\n\\n“You shan’t be beheaded!” said Alice, and she put them into a large\\nflower-pot that stood near. The three soldiers wandered about for a\\nminute or two, looking for them, and then quietly marched off after the\\nothers.\\n\\n“Are their heads off?” shouted the Queen.'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answer['source_documents'][0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mbw38Al09FlS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "“Leave off that!” screamed the Queen. “You make me giddy.” And then,\n",
            "turning to the rose-tree, she went on, “What _have_ you been doing\n",
            "here?”\n",
            "\n",
            "“May it please your Majesty,” said Two, in a very humble tone, going\n",
            "down on one knee as he spoke, “we were trying—”\n",
            "\n",
            "“_I_ see!” said the Queen, who had meanwhile been examining the roses.\n",
            "“Off with their heads!” and the procession moved on, three of the\n",
            "soldiers remaining behind to execute the unfortunate gardeners, who ran\n",
            "to Alice for protection.\n",
            "\n",
            "“You shan’t be beheaded!” said Alice, and she put them into a large\n",
            "flower-pot that stood near. The three soldiers wandered about for a\n",
            "minute or two, looking for them, and then quietly marched off after the\n",
            "others.\n",
            "\n",
            "“Are their heads off?” shouted the Queen.\n"
          ]
        }
      ],
      "source": [
        "print(answer['source_documents'][0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Uzcci0pHI43"
      },
      "source": [
        "The documents name also gets returned, useful if you have multiple documents!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "mzqDKDaAAA2N"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'source': '/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/alice_in_wonderland.txt'}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answer['source_documents'][0].metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "8cK_y5OcAKu5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/alice_in_wonderland.txt'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answer['source_documents'][0].metadata[\"source\"]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
